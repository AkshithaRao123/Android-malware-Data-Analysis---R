# source of dataset:
# https://www.kaggle.com/datasets/shashwatwork/android-malware-dataset-for-machine-learning

# Set working directory to an appropriate location.
setwd("C:/Users/Ashwin-Rao/Desktop/prjct")

data = read.csv("drebin-215-dataset-5560malware-9476-benign.csv")
#print(data)
print(summary(data$onServiceConnected))
# Checking for categorical values
print(summary(data))
print(summary(data$TelephonyManager.getSimCountryIso))
print(str(data, list.len = ncol(data)))

count = 0
for(i in data$TelephonyManager.getSimCountryIso)
{
  if(i != 0 && i!=1)
  {
    count = count+1
  }
}
print(count)

# Column: TelephonyManager.getSimCountryIso has character values,
# apart from the class column, where B means Benign and S means Malware
# TelephonyManager.getSimCountryIso is supposed to have 0s and 1s
# for FALSE and TRUE respectively
aux = subset(data, select = TelephonyManager.getSimCountryIso)
#print(aux)
#write.csv(aux, "TelManagerSimCountry.csv", row.names = FALSE)

# Checking and removing missing or trash values
lev = factor(aux$TelephonyManager.getSimCountryIso)
nlevels(lev)
print(lev)
# This shows 3 levels: '0', '1' and '?'

count = 0
for(i in aux$TelephonyManager.getSimCountryIso)
{
  if(i != '0' && i!='1')
  {
    count = count+1
  }
}
print(paste("No. of '?': ",count))
# Count shows 5, which means apart from 0 and 1, there are 5 '?' in that column

library(tidyr)
library(dplyr)

# Convert '?' to NA and delete rows with NA, 
# to avoid rows with missing information
replacedValues = replace(data , data == '?' , NA)
print(replacedValues)
print(summary(replacedValues))
write.csv(replacedValues, "replacedValues.csv", row.names = FALSE)
print(summary(replacedValues$TelephonyManager.getSimCountryIso))

tidiedDf = replacedValues %>% drop_na()
print(str(tidiedDf, list.len = ncol(data)))

#write.csv(tidiedDf, "tidiedDf.csv", row.names = FALSE)

# JUST TO TEST
aux = subset(tidiedDf, select = TelephonyManager.getSimCountryIso)
test = c(as.integer(as.character(aux$TelephonyManager.getSimCountryIso)))

# TYPECASTING TelephonyManager.getSimCountryIso TO INTEGER
tidiedDf$TelephonyManager.getSimCountryIso = test
print(str(tidiedDf, list.len = ncol(data)))
print(summary(tidiedDf$TelephonyManager.getSimCountryIso))
write.csv(tidiedDf, "tidiedDf.csv", row.names = FALSE)
print(summary(tidiedDf$TelephonyManager.getSimCountryIso))

# CHECKING PROPORTIONALITY BETWEEN SUSPICIOUS AND BENIGN DATA
table(tidiedDf$class)
barplot(prop.table(table(tidiedDf$class)),
        col = c("green","red"),
        ylab = "No. of samples",
        ylim = c(0,0.8),
        main = "Class Distribution",
        sub = "B - Benign, S - Suspicious")
# There are too less details of Malwares when compared to Benign softwares
# Data is not balanced yet

# ENCODING CATEGORICAL DATA
tidiedDf <- mutate(tidiedDf, Suspicious = ifelse(class == "S",1L,0L))
print(str(tidiedDf, list.len = ncol(tidiedDf)))
print(tidiedDf) 
write.csv(tidiedDf,"FINALCSVDF.csv", row.names = FALSE)

# DROP COLUMN 'class'
undesired <- c('class')
tidiedDf <- tidiedDf %>%
  select(-any_of(undesired))
print(str(tidiedDf, list.len = ncol(tidiedDf)))

# PARTITIONING DATA
set.seed(502)
ind = sample(2, nrow(tidiedDf), replace = TRUE, prob = c(0.7, 0.3))
train <- tidiedDf[ind==1, ]
test <- tidiedDf[ind==2, ]
print(str(test, list.len = ncol(tidiedDf)))
table(train$Suspicious)

# Synthetic Balancing of Data
library(ROSE)
balancedData = ovun.sample(Suspicious~., data = train, method = "both",
                   p = 0.5, seed = 222, N = 10658)$data
table(balancedData$Suspicious)

# To separate predictor variables and labels
trainNoLabels <- balancedData %>% select(-c("Suspicious"))
testNoLabels <- test %>% select(-c("Suspicious"))
trainLabels <- subset(balancedData, select = Suspicious)
testLabels <- subset(test, select = Suspicious)

# ANALYSIS OF TRAINING DATASET
library(xgboost)
library(caret)

dtrain <- xgb.DMatrix(data = as.matrix(trainNoLabels), 
                      label= trainLabels[1:nrow(trainNoLabels), ])
dtrain

negative_cases <- sum(trainLabels == FALSE)
postive_cases <- sum(trainLabels == TRUE)

model <- xgboost(data = dtrain, 
                 nround = 10, 
                 objective = "binary:logistic",
                 early_stopping_rounds = 3,
                 scale_pos_weight = negative_cases/postive_cases) 

dtest <- xgb.DMatrix(data = as.matrix(testNoLabels), 
                      label= testLabels[1:nrow(testNoLabels), ])

pred <- predict(model, dtest)

# Get & print the classification error
err <- mean(as.numeric(pred > 0.5) != testLabels)
print(paste("test-error=", err))

# Plot the 20 most important deciding features and save it
importance_matrix <- xgb.importance(names(as.matrix(trainNoLabels)), model = model)
png(file = "Impplot.png")
xgb.plot.importance(importance_matrix, col = "violet",
                    top_n = 20, cex = 0.9)
dev.off()
library("DiagrammeR")

xgb.plot.tree(model = model, trees = 1)
png(file = "ShapSummary.png")
xgb.plot.shap.summary(as.matrix(trainNoLabels), model = model)
dev.off()
